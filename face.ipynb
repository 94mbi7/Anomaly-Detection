{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c00f594-4ddf-4ed4-b4fe-57411ad639f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 656ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Invalid predicted_label: 4. Check classes_list length.\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf  # Assuming you are using TensorFlow for the model\n",
    "\n",
    "# Load your pre-trained model\n",
    "model = tf.keras.models.load_model('model_8000.h5')\n",
    "\n",
    "# Define classes_list (replace with your actual list of classes)\n",
    "classes_list = [\"Abuse\",\"Arrest\",\"Arson\",\"Assault\"]\n",
    "\n",
    "# Define image dimensions\n",
    "image_height = 64\n",
    "image_width = 64\n",
    "\n",
    "def save_clip(frames, output_file_path):\n",
    "    if not frames:\n",
    "        return\n",
    "\n",
    "    height, width, _ = frames[0].shape\n",
    "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 24, (width, height))\n",
    "\n",
    "    for frame in frames:\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()\n",
    "\n",
    "def predict_on_live_video(video_file_path, output_file_path, window_size):\n",
    "    frame_count = 0\n",
    "    frames = []\n",
    "    predicted_label = 4\n",
    "    predicted_labels_probabilities_deque = deque(maxlen=window_size)\n",
    "\n",
    "    video_reader = cv2.VideoCapture(video_file_path)\n",
    "\n",
    "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 24, (original_video_width, original_video_height))\n",
    "\n",
    "    while True:\n",
    "        status, frame = video_reader.read()\n",
    "\n",
    "        if not status:\n",
    "            break  # This break statement is inside the while loop\n",
    "\n",
    "        resized_frame = cv2.resize(frame, (image_height, image_width))\n",
    "        normalized_frame = resized_frame / 255\n",
    "\n",
    "        predicted_labels_probabilities = model.predict(np.expand_dims(normalized_frame, axis=0))[0]\n",
    "\n",
    "        predicted_labels_probabilities_deque.append(predicted_labels_probabilities)\n",
    "\n",
    "        if len(predicted_labels_probabilities_deque) == window_size:\n",
    "            predicted_labels_probabilities_np = np.array(predicted_labels_probabilities_deque)\n",
    "            predicted_labels_probabilities_averaged = predicted_labels_probabilities_np.mean(axis=0)\n",
    "\n",
    "            predicted_max = np.max(predicted_labels_probabilities_averaged)\n",
    "            predicted_label = np.argmax(predicted_labels_probabilities_averaged)\n",
    "\n",
    "            if predicted_max > 0.6 and predicted_label < 4:\n",
    "                frame_count += 1\n",
    "                if frame_count < 450:\n",
    "                    frames.append(frame)\n",
    "                else:\n",
    "                    if len(frames) > 100:\n",
    "                        save_clip(frames, output_file_path)\n",
    "                    frame_count = 0\n",
    "                    frames = []\n",
    "            else:\n",
    "                predicted_label = 4\n",
    "\n",
    "            if predicted_label < len(classes_list):  # Ensure predicted_label is within the valid range\n",
    "                predicted_class_name = classes_list[predicted_label]\n",
    "                cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            else:\n",
    "                print(f\"Invalid predicted_label: {predicted_label}. Check classes_list length.\")\n",
    "\n",
    "        video_writer.write(frame)\n",
    "        cv2.imshow('Predicted Frames', frame)\n",
    "\n",
    "        key_pressed = cv2.waitKey(10)\n",
    "\n",
    "        if key_pressed == ord('q'):\n",
    "            break\n",
    "            \n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video_reader.release()\n",
    "    video_writer.release()\n",
    "\n",
    "# Example usage\n",
    "output_file_path = \"C:/Users/ryzen/Downloads/VID-20240510-WA0004.mp4\"\n",
    "window_size = 30\n",
    "video_file_path = \"C:/Users/ryzen/seeya/video.mp4\"\n",
    "predict_on_live_video(video_file_path, output_file_path, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d09905-3c88-4075-a97e-560507994d7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (556306433.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install opencv-python\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9363e925",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m----> 2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[43my_true\u001b[49m, y_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff4ab74-52ea-4ff6-bff7-0cec6a1542d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 64)        1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 60, 60, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 30, 30, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 64)                0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               16640     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57668 (225.27 KB)\n",
      "Trainable params: 57028 (222.77 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model_8000.h5')\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b904fa4-64f0-4d86-82f1-110b41ab8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#1. Convolutional layer (`Conv2D`) with 64 filters and a kernel size of 3x3, resulting in an output shape of (None, 62, 62, 64).\n",
    "#2. Convolutional layer (`Conv2D`) with 64 filters and a kernel size of 3x3, resulting in an output shape of (None, 60, 60, 64).\n",
    "#3. Batch normalization layer (`BatchNormalization`) with an output shape of (None, 60, 60, 64).\n",
    "#4. Max pooling layer (`MaxPooling2D`) with a pool size of 2x2, resulting in an output shape of (None, 30, 30, 64).\n",
    "#5. Global average pooling layer (`GlobalAveragePooling2D`) with an output shape of (None, 64).\n",
    "#6. Dense layer (`Dense`) with 256 units, resulting in an output shape of (None, 256).\n",
    "#7. Batch normalization layer (`BatchNormalization`) with an output shape of (None, 256).\n",
    "#8. Dense layer (`Dense`) with 4 units, representing the output classes.\n",
    "\n",
    "#So, there are a total of **7 layers** in the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Local Connectivity: CNNs use convolutional layers that apply filters (kernels) to local regions of the image, \n",
    "# capturing local patterns such as edges, textures, and simple shapes. \n",
    "\n",
    "#2. Shared Weights: The same filter is applied across different parts of the image, which allows the network to learn translation-invariant features. \n",
    "# This means the network can recognize patterns regardless of their position in the image.\n",
    "\n",
    "#3. Hierarchical Feature Learning: CNNs build hierarchical feature representations. Lower layers capture simple features (edges, textures),\n",
    "#  while deeper layers combine these features to recognize more complex patterns (object parts, objects). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
